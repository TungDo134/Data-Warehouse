# CÃ i Ä‘áº·t thÆ° viá»‡n
import pandas as pd
import os
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from datetime import datetime
from zoneinfo import ZoneInfo

from db_utils import load_to_mysql

# from selenium.webdriver.common.by import By
# from unidecode import unidecode
# from webdriver_manager.chrome import ChromeDriverManager




# ===================== âš™ï¸ Cáº¤U HÃŒNH BAN Äáº¦U =====================

category_url = "https://www.thegioididong.com/dtdd"

chrome_options = Options()
chrome_options.add_argument("--headless=new")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("window-size=1920,1080")

driver = webdriver.Chrome(options=chrome_options)

# ===================== ğŸ”¹ BÆ¯á»šC 1: Láº¤Y LINK + TÃŠN + GIÃ =====================

print(f"ğŸ” Äang thu tháº­p danh sÃ¡ch sáº£n pháº©m tá»«: {category_url}")
driver.get(category_url)
time.sleep(3)

# --- Tá»± Ä‘á»™ng click "Xem thÃªm" (giá»›i háº¡n sá»‘ láº§n thá»­) ---
max_clicks = 1
for i in range(max_clicks):
    try:
        view_more_btn = driver.find_element("css selector", ".view-more a")
        driver.execute_script("arguments[0].scrollIntoView(true);", view_more_btn)
        time.sleep(1)
        driver.execute_script("arguments[0].click();", view_more_btn)
        print(f"ğŸ” (Láº§n {i+1}/{max_clicks}) ÄÃ£ click 'Xem thÃªm' Ä‘á»ƒ táº£i thÃªm sáº£n pháº©m...")
        time.sleep(random.uniform(2.5, 4.5))
    except Exception:
        print(f"âœ… Dá»«ng á»Ÿ láº§n {i+1}: KhÃ´ng cÃ²n nÃºt 'Xem thÃªm' hoáº·c Ä‘Ã£ load háº¿t.")
        break
else:
    print("âš ï¸ ÄÃ£ Ä‘áº¡t giá»›i háº¡n click tá»‘i Ä‘a, cÃ³ thá»ƒ trang chÆ°a load háº¿t.")

# --- Sau khi Ä‘Ã£ táº£i háº¿t ---
soup = BeautifulSoup(driver.page_source, "lxml")
product_links = []
products = []

for a in soup.select("ul.listproduct a.main-contain"):
    href = a.get("href")
    if href and href.startswith("/dtdd/"):
        full_link = "https://www.thegioididong.com" + href
        product_links.append(full_link)

        # --- TÃªn sáº£n pháº©m ---
        name_tag = a.select_one("h3")
        product_name = name_tag.get_text(strip=True) if name_tag else "KhÃ´ng rÃµ"

        # --- GiÃ¡ sáº£n pháº©m ---
        price_tag = a.select_one("strong.price")
        if price_tag:
            price_text = price_tag.get_text(strip=True).replace("â‚«", "").replace(".", "").strip()
            try:
                price = int(price_text)
            except ValueError:
                price = None
        else:
            price = None

        products.append({
            "TÃªn sáº£n pháº©m": product_name,
            "GiÃ¡": price,
            "Link": full_link
        })

print(f"âœ… TÃ¬m tháº¥y {len(product_links)} sáº£n pháº©m sau khi load toÃ n trang.")


# âš™ï¸ Giá»›i háº¡n sá»‘ lÆ°á»£ng sáº£n pháº©m Ä‘á»ƒ TEST (muá»‘n full thÃ¬ cmt láº¡i)
limit = 10
product_links = product_links[:limit]
products = products[:limit]
print(f"ğŸ§ª Äang test vá»›i {len(products)} sáº£n pháº©m Ä‘áº§u tiÃªn.")

# ===================== ğŸ”¹ BÆ¯á»šC 2: CRAWL CHI TIáº¾T =====================

all_data = []

# HÃ m giÃºp phá»¥c há»“i khi gáº·p lá»—i máº¡ng hoáº·c trang bá»‹ treo
# GiÃºp ctrinh váº«n cháº¡y tiáº¿p thay vÃ¬ break
def safe_get(url, retries=3):
    """Táº£i trang vá»›i retry vÃ  timeout (áº©n traceback, chá»‰ in lá»—i gá»n)."""
    for attempt in range(retries):
        try:
            driver.set_page_load_timeout(20)
            driver.get(url)
            time.sleep(random.uniform(2.5, 4.5))
            return BeautifulSoup(driver.page_source, "lxml")
        except Exception as e:
            print(f"âš ï¸ Lá»—i táº£i ({attempt+1}/{retries})")
            if attempt == retries - 1:
                print(f"âŒ Bá» qua {url}")
                return None
            time.sleep(2)
    return None

# Logic Ä‘á»ƒ láº¥y chi tiáº¿t sáº£n pháº©m
for i, base_info in enumerate(products, start=1):
    url = base_info["Link"]
    print(f"ğŸ“¦ ({i}/{len(products)}) Äang xá»­ lÃ½: {url}")

    soup = safe_get(url)
    if not soup:
        continue

    config = base_info.copy()
    for item in soup.select("ul.text-specifi li"):
        label_tag = item.find("strong") or item.find("a")
        label = label_tag.get_text(strip=True) if label_tag else None

        value_tags = item.select("span, a")
        values = [v.get_text(strip=True) for v in value_tags if v.get_text(strip=True)]

        if label and values and values[0] == label:
            values = values[1:]

        if label and values:
            config[label] = " | ".join(values)

    all_data.append(config)

print(f"ğŸ¯ ÄÃ£ thu tháº­p Ä‘Æ°á»£c {len(all_data)} sáº£n pháº©m há»£p lá»‡.")
driver.quit()

# ===================== ğŸ”¹ BÆ¯á»šC 3: Táº O + LÆ¯U FILE EXCEL =====================

# ThÆ° má»¥c báº¡n muá»‘n lÆ°u file
output_dir = r"D:\Workspace-Python\Data-Warehouse\Crawl Data"

# Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i
os.makedirs(output_dir, exist_ok=True)
df = pd.DataFrame(all_data)

# xÃ³a cá»™t khÃ´ng cáº§n thiáº¿t
df = df.drop(columns=["Tháº» nhá»›:", "Sáº¡c kÃ¨m theo mÃ¡y:","Radio:","ÄÃ¨n pin:",
                      "KÃ­ch thÆ°á»›c mÃ n hÃ¬nh:"], errors='ignore')

now_vn = datetime.now(ZoneInfo("Asia/Ho_Chi_Minh"))
timestamp = now_vn.strftime("%Y_%m_%d_%H_%M_%S")

# Táº¡o tÃªn file cÃ³ timestamp
filename = os.path.join(output_dir, f"tgdd_products_{timestamp}.xlsx")

df.to_excel(filename, index=False)
print(f"ğŸ‰ Crawl hoÃ n táº¥t. ÄÃ£ lÆ°u file: {filename}")

# load vÃ o mysql
table_name = "staging.rawtgdd"
load_to_mysql(df, table_name, os.path.basename(filename))
