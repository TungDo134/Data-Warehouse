import os
import re
import yaml
import pandas as pd
import mysql.connector
from datetime import datetime
from zoneinfo import ZoneInfo
from unidecode import unidecode
from sqlalchemy import create_engine, text

# Import log utilities (gi·∫£ s·ª≠ c√≥ file utils/log_utils.py)
try:
    from utils.log_utils import write_log
except ImportError:
    # Fallback n·∫øu ch∆∞a c√≥ module log_utils
    def write_log(process_name, source_system, target_table, status, message="", rows=None, product_data=None):
        """H√†m log d·ª± ph√≤ng - ghi v√†o database"""
        log_time = datetime.now()
        try:
            engine = create_engine("mysql+mysqlconnector://root:@localhost:3306/product_logdb")
            insert_log_sql = text("""
                                  INSERT INTO etl_logs(process_name, source_system, target_table, status, message,
                                                       rows_affected, log_time)
                                  VALUES (:process_name, :source_system, :target_table, :status, :message,
                                          :rows_affected,
                                          :log_time)
                                  """)
            with engine.begin() as conn:
                conn.execute(insert_log_sql, {
                    "process_name": process_name,
                    "source_system": source_system,
                    "target_table": target_table,
                    "status": status,
                    "message": message,
                    "rows_affected": rows,
                    "log_time": log_time
                })
        except Exception as e:
            print(f"‚ùå Failed to write log: {e}")

# ======================CONFIG ======================
structure_path = r"D:\Workspace-Python\Data-Warehouse\Data Warehouse.xlsx"
table_name = "dim_products"
source_label = "TGDD"

# 5.3.1.0 K·∫æT N·ªêI DB
def get_connection():
    return mysql.connector.connect(
        host="localhost",
        user="root",
        password="",
        database="data_storage"
    )

# 5.3.1.1 ƒê·ªçc config t·ª´ YAML
def load_db_config(path="config/db_config.yaml"):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)["mysql"]


# ======================T·∫†O B·∫¢NG DIM T·ª™ EXCEL ======================
# 5.3.1.2 T·∫†O B·∫¢NG DIM T·ª™ EXCEL
def create_table_from_excel(excel_path, sheet_name, conn, db_name="data_storage"):
    """T·∫°o b·∫£ng DIM t·ª´ c·∫•u tr√∫c Excel"""
    try:
        print("\nüìã ƒêang ƒë·ªçc c·∫•u tr√∫c b·∫£ng t·ª´ Excel...")

        df_struct = pd.read_excel(excel_path, sheet_name=sheet_name, usecols="A:C", header=9)
        df_struct.columns = [c.strip().lower().replace(" ", "_") for c in df_struct.columns]
        col_field = next((c for c in df_struct.columns if "field" in c), None)
        col_type = next((c for c in df_struct.columns if "type" in c), None)
        df_struct = df_struct[df_struct[col_field].notna()]

        sql_cols = ["`id` INT NOT NULL AUTO_INCREMENT"]
        pk_cols = ["id"]

        for _, row in df_struct.iterrows():
            field = str(row[col_field]).strip()
            dtype = str(row[col_type]).strip().upper()

            if field == "product_id":
                dtype = "VARCHAR(50)"
                sql_cols.append(f"`{field}` {dtype} NOT NULL UNIQUE")
            elif dtype in ["VARCHAR", "TEXT"] or field in ["product_name", "operating_system", "cpu_chip", "cpu_speed"]:
                dtype = "TEXT"
                sql_cols.append(f"`{field}` {dtype} NULL")
            elif field == "product_price":
                dtype = "DECIMAL(18,2)"
                sql_cols.append(f"`{field}` {dtype} NULL")
            elif field == "release_date":
                dtype = "DATE"
                sql_cols.append(f"`{field}` {dtype} NULL")
            else:
                sql_cols.append(f"`{field}` {dtype} NULL")

        pk_clause = f", PRIMARY KEY ({','.join([f'`{c}`' for c in pk_cols])})"
        create_sql = f"CREATE TABLE IF NOT EXISTS `{db_name}`.`{sheet_name}` ({', '.join(sql_cols)} {pk_clause}) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;"

        print(f"‚úÖ SQL sinh ra th√†nh c√¥ng")
        with conn.cursor() as cursor:
            cursor.execute(create_sql)
            conn.commit()

        print(f"‚úÖ B·∫£ng `{sheet_name}` ƒë√£ ƒë∆∞·ª£c t·∫°o/ki·ªÉm tra trong database `{db_name}`")

        write_log(
            process_name="CREATE_TABLE_DIM",
            source_system=source_label,
            target_table=sheet_name,
            status="SUCCESS",
            message=f"T·∫°o/ki·ªÉm tra b·∫£ng {sheet_name} th√†nh c√¥ng"
        )

    except Exception as e:
        error_msg = f"L·ªói khi t·∫°o b·∫£ng {sheet_name}: {str(e)}"
        print(f"‚ùå {error_msg}")

        write_log(
            process_name="CREATE_TABLE_DIM",
            source_system=source_label,
            target_table=sheet_name,
            status="FAILED",
            message=error_msg
        )
        raise


# ====================== ƒê·ªåC STAGING ======================
# 5.3.1.3 ƒê·ªåC STAGING
def read_staging_from_db():
    """ƒê·ªçc d·ªØ li·ªáu t·ª´ staging database"""
    try:
        print("\nüì• ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ staging...")

        DB_CONFIG = load_db_config()
        conn_str = f"mysql+mysqlconnector://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}/{DB_CONFIG['database']}"
        engine = create_engine(conn_str)
        df = pd.read_sql("SELECT * FROM `staging`.`staging.rawtgdd`", engine)
        engine.dispose()

        # 5.3.1.4 Chu·∫©n h√≥a t√™n c·ªôt staging
        df.columns = [re.sub(r'[^a-z0-9]+', '_', unidecode(c).lower()).strip('_') for c in df.columns]

        print(f"‚úÖ ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ staging.rawtgdd")
        print(f"üìä C·ªôt staging: {df.columns.tolist()[:5]}...")

        write_log(
            process_name="READ_STAGING",
            source_system=source_label,
            target_table="staging.rawtgdd",
            status="SUCCESS",
            rows=len(df),
            message=f"ƒê·ªçc staging th√†nh c√¥ng v·ªõi {len(df)} d√≤ng"
        )

        return df

    except Exception as e:
        error_msg = f"L·ªói khi ƒë·ªçc staging: {str(e)}"
        print(f"‚ùå {error_msg}")

        write_log(
            process_name="READ_STAGING",
            source_system=source_label,
            target_table="staging.rawtgdd",
            status="FAILED",
            message=error_msg
        )
        raise


# ======================L·∫§Y C·ªòT DIM ======================
# 5.3.1.5 L·∫§Y C·ªòT DIM
def get_table_columns(conn, table_name):
    """L·∫•y danh s√°ch c·ªôt t·ª´ b·∫£ng DIM"""
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"SHOW COLUMNS FROM data_storage.{table_name};")
            cols = [row[0] for row in cursor.fetchall()]

        print(f"‚úÖ ƒê√£ ƒë·ªçc {len(cols)} c·ªôt t·ª´ {table_name}")
        return cols

    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc c·ªôt t·ª´ {table_name}: {e}")
        raise


# ====================== LOAD TO MYSQL ======================
# 5.3.1.10 LOAD TO MYSQL (UPSERT)
def load_to_mysql(df, table_name):
    """Load d·ªØ li·ªáu v√†o MySQL v·ªõi INSERT ... ON DUPLICATE KEY UPDATE"""
    try:
        print(f"\nüíæ ƒêang load {len(df)} d√≤ng v√†o {table_name}...")

        DB_CONFIG = load_db_config()
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()

        cols = [c for c in df.columns if c != "id"]
        placeholders = ",".join(["%s"] * len(cols))
        col_names = ",".join([f"`{c}`" for c in cols])
        update_clause = ",".join([f"`{c}`=VALUES(`{c}`)" for c in cols if c != "id"])

        sql = f"""
        INSERT INTO `data_storage`.`{table_name}` ({col_names})
        VALUES ({placeholders})
        ON DUPLICATE KEY UPDATE {update_clause}
        """

        success_count = 0
        error_count = 0

        for _, row in df.iterrows():
            values = [row[c] for c in cols]
            try:
                cursor.execute(sql, values)
                success_count += 1
            except Exception as e:
                error_count += 1
                print(f"‚ö†Ô∏è L·ªói khi ghi d√≤ng {row.get('product_name', '')}: {e}")

        conn.commit()
        cursor.close()
        conn.close()

        print(f"‚úÖ ƒê√£ n·∫°p {success_count} d√≤ng v√†o {table_name}")
        if error_count > 0:
            print(f"‚ö†Ô∏è {error_count} d√≤ng b·ªã l·ªói")

        write_log(
            process_name="LOAD_DIM_MYSQL",
            source_system=source_label,
            target_table=table_name,
            status="SUCCESS",
            rows=success_count,
            message=f"Load th√†nh c√¥ng {success_count}/{len(df)} d√≤ng v√†o {table_name}"
        )

    except Exception as e:
        error_msg = f"L·ªói khi load v√†o MySQL: {str(e)}"
        print(f"‚ùå {error_msg}")

        write_log(
            process_name="LOAD_DIM_MYSQL",
            source_system=source_label,
            target_table=table_name,
            status="FAILED",
            message=error_msg
        )
        raise


# ======================MAPPING C·ª®NG ======================
# 5.3.1.6 MANUAL MAPPING
manual_mapping = {
    "product_name": "ten_san_pham",
    "product_price": "gia",
    "operating_system": "he_dieu_hanh",
    "cpu_chip": "chip_xu_ly_cpu",
    "cpu_speed": "toc_do_cpu",
    "gpu_chip": "chip_do_hoa_gpu",
    "ram": "ram",
    "storage_capacity": "dung_luong_luu_tru",
    "available_storage": "dung_luong_con_lai_kha_dung_khoang",
    "contacts": "danh_ba",
    "rear_camera_resolution": "do_phan_giai_camera_sau",
    "rear_camera_video": "quay_phim_camera_sau",
    "rear_camera_flash": "den_flash_camera_sau",
    "rear_camera_features": "tinh_nang_camera_sau",
    "front_camera_resolution": "do_phan_giai_camera_truoc",
    "front_camera_features": "tinh_nang_camera_truoc",
    "display_technology": "cong_nghe_man_hinh",
    "display_resolution": "do_phan_giai_man_hinh",
    "screen_size": "man_hinh_rong",
    "max_brightness": "do_sang_toi_da",
    "touch_glass": "mat_kinh_cam_ung",
    "battery_capacity": "dung_luong_pin",
    "battery_type": "loai_pin",
    "max_charging_support": "ho_tro_sac_toi_da",
    "battery_technology": "cong_nghe_pin",
    "security_features": "bao_mat_nang_cao",
    "special_features": "tinh_nang_dac_biet",
    "water_dust_resistance": "khang_nuoc_bui",
    "voice_recorder": "ghi_am",
    "video_playback": "xem_phim",
    "music_playback": "nghe_nhac",
    "mobile_network": "mang_di_dong",
    "sim_type": "sim",
    "wifi_support": "wifi",
    "gps_support": "gps",
    "bluetooth_version": "bluetooth",
    "cong_ket_noi/sac": "cong_ket_noi_sac",
    "headphone_jack": "jack_tai_nghe",
    "other_connections": "ket_noi_khac",
    "design_style": "thiet_ke",
    "material": "chat_lieu",
    "dimensions_weight": "kich_thuoc_khoi_luong",
    "release_date": "thoi_diem_ra_mat",
    "brand": "hang"
}

# ======================MAIN ETL ======================

if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ B·∫ÆT ƒê·∫¶U LOAD DIM_PRODUCT")
    print("=" * 60)

    # Ghi log b·∫Øt ƒë·∫ßu
    write_log(
        process_name="LOAD_DIM_START",
        source_system=source_label,
        target_table=table_name,
        status="RUNNING",
        message="B·∫Øt ƒë·∫ßu qu√° tr√¨nh load DIM_PRODUCT"
    )

    try:
        # ========== B∆Ø·ªöC 1: T·∫†O B·∫¢NG ==========
        # 5.3.1.0 K·∫øt n·ªëi DB
        conn = get_connection()
        # 5.3.1.2 T·∫°o b·∫£ng DIM
        create_table_from_excel(structure_path, table_name, conn)

        # ========== B∆Ø·ªöC 2: ƒê·ªåC STAGING ==========
        # 5.3.1.3 ƒê·ªçc staging
        df_staging = read_staging_from_db()

        # ========== B∆Ø·ªöC 3: L·∫§Y C·∫§U TR√öC DIM ==========
        # 5.3.1.5 ƒê·ªçc c·∫•u tr√∫c c·ªôt DIM
        dim_fields = get_table_columns(conn, table_name)

        # ========== B∆Ø·ªöC 4: MAPPING D·ªÆ LI·ªÜU ==========

        print("\nüîÑ ƒêang mapping d·ªØ li·ªáu t·ª´ staging sang DIM...")

        # 5.3.1.6 B·∫Øt ƒë·∫ßu Mapping
        df_dim = pd.DataFrame(columns=dim_fields)
        df_staging.columns = [c.strip().lower() for c in df_staging.columns]

        for dim_col in dim_fields:
            stg_col = manual_mapping.get(dim_col)
            if stg_col and stg_col in df_staging.columns:
                df_dim[dim_col] = df_staging[stg_col]
            else:
                df_dim[dim_col] = None

        original_count = len(df_dim)

        # ========== B∆Ø·ªöC 5: CLEAN D·ªÆ LI·ªÜU ==========
        print("\nüßπ ƒêang clean d·ªØ li·ªáu...")
        # 5.3.1.7 X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu
        required_cols = ["release_date", "product_name", "product_price"]
        df_dim = df_dim.dropna(subset=required_cols)

        df_dim["release_date"] = pd.to_datetime(
            df_dim["release_date"], errors="coerce", format="%m/%Y"
        )
        # chuy·ªÉn release_date
        df_dim = df_dim[df_dim["release_date"].notna()]
        df_dim["release_date"] = df_dim["release_date"].dt.strftime("%Y-%m-%d")
        # t·∫°o product_id
        if "product_id" in df_dim.columns:
            df_dim["product_id"] = df_dim["product_id"].fillna(
                pd.Series([f"P{i:05d}" for i in range(1, len(df_dim) + 1)], index=df_dim.index)
            )
        # 5.3.1.8 Th√™m metadata
        now_vn = datetime.now(ZoneInfo("Asia/Ho_Chi_Minh"))
        if "dt_expired" in df_dim.columns:
            df_dim["dt_expired"] = now_vn.strftime("%Y-%m-%d %H:%M:%S")
        if "source_file" in df_dim.columns:
            df_dim["source_file"] = source_label
        # 5.3.1.9 Chuy·ªÉn NaN ‚Üí None
        df_dim = df_dim.where(pd.notnull(df_dim), None)

        cleaned_count = len(df_dim)
        removed_count = original_count - cleaned_count
        print(f"‚úÖ Clean ho√†n t·∫•t: Gi·ªØ l·∫°i {cleaned_count}/{original_count} d√≤ng (lo·∫°i b·ªè {removed_count} d√≤ng)")

        write_log(
            process_name="TRANSFORM_DIM",
            source_system=source_label,
            target_table=table_name,
            status="SUCCESS",
            rows=cleaned_count,
            message=f"Mapping v√† clean th√†nh c√¥ng. Lo·∫°i b·ªè {removed_count} d√≤ng kh√¥ng h·ª£p l·ªá"
        )

        # ========== B∆Ø·ªöC 6: XU·∫§T FILE EXCEL ==========
        print("\nüìÑ ƒêang xu·∫•t file Excel...")
        # 5.3.1.10 Xu·∫•t Excel DIM
        output_dir = r"D:\Workspace-Python\Data-Warehouse\Data_storage_DIM"
        os.makedirs(output_dir, exist_ok=True)

        timestamp = now_vn.strftime("%Y_%m_%d_%H_%M_%S")
        output_file = os.path.join(output_dir, f"dim_product_{timestamp}.xlsx")

        df_dim.to_excel(output_file, index=False)
        print(f"‚úÖ ƒê√£ xu·∫•t file: {output_file}")

        write_log(
            process_name="EXPORT_DIM_EXCEL",
            source_system=source_label,
            target_table=table_name,
            status="SUCCESS",
            rows=cleaned_count,
            message=f"Xu·∫•t file Excel th√†nh c√¥ng: {os.path.basename(output_file)}"
        )

        # ========== B∆Ø·ªöC 7: LOAD V√ÄO MYSQL ==========
        # 5.3.1.11 LOAD v√†o MYSQL
        load_to_mysql(df_dim, table_name)

        # ========== HO√ÄN TH√ÄNH ==========
        print("\n" + "=" * 60)
        print(f"‚úÖ HO√ÄN T·∫§T LOAD DIM_PRODUCT - X·ª≠ l√Ω {cleaned_count} s·∫£n ph·∫©m")
        print("=" * 60)

        write_log(
            process_name="LOAD_DIM_COMPLETE",
            source_system=source_label,
            target_table=table_name,
            status="SUCCESS",
            rows=cleaned_count,
            message=f"Load DIM_PRODUCT ho√†n th√†nh th√†nh c√¥ng v·ªõi {cleaned_count} s·∫£n ph·∫©m"
        )

    except Exception as e:
        error_msg = f"L·ªói trong qu√° tr√¨nh load DIM: {str(e)}"
        print(f"\n‚ùå {error_msg}")

        write_log(
            process_name="LOAD_DIM_FAILED",
            source_system=source_label,
            target_table=table_name,
            status="FAILED",
            message=error_msg
        )

        raise